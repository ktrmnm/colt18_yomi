# COLT2018 全部読み会

## 目的

機械学習の理論系トップ会議であるCOLT2018に採択された論文を全部読みます (全部読むとは言っていない)。

トップ会議の論文を把握し、研究テーマや傾向についての勘をつかむのが目標です。

## 方法

* Issuesに要約を書く。
* 数式はGitHubでMathJaxをレンダリングする[Chrome拡張](https://chrome.google.com/webstore/detail/github-with-mathjax/ioemnmodlmafdkllaclgeombjnmnbima)で読めるように書く。他の読み方は想定しない。
* 概要を把握したら下記のリストにチェック

## Accepter Papers

- [Accepted Papers](http://www.learningtheory.org/colt2018/call.html#accepted-papers)
- [PMLR](http://proceedings.mlr.press/v75/)

* [ ] Actively Avoiding Nonsense in Generative Models
* [ ] A Faster Approximation Algorithm for the Gibbs Partition Function
* [ ] Exponential convergence of testing error for stochastic gradient methods
* [ ] Size-Independent Sample Complexity of Neural Networks
* [ ] Underdamped Langevin MCMC: A non-asymptotic analysis
* [ ] Online Variance Reduction for Stochastic Optimization
* [ ] Information Directed Sampling and Bandits with Heteroscedastic Noise
* [ ] Testing Symmetric Markov Chains From a Single Trajectory
* [ ] Detection limits in the high-dimensional spiked rectangular model
* [ ] Learning Without Mixing: Towards A Sharp Analysis of Linear System Identification
* [ ] Active Tolerant Testing
* [ ] Polynomial Time and Sample Complexity for Non-Gaussian Component Analysis: Spectral Methods
* [ ] Calibrating Noise to Variance in Adaptive Data Analysis
* [ ] Accelerating Stochastic Gradient Descent for Least Squares Regression
* [ ] Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints
* [ ] Optimal approximation of continuous functions by very deep ReLU networks
* [ ] Averaged Stochastic Gradient Descent on Riemannian Manifolds
* [ ] Fitting a putative manifold to noisy data
* [ ] Private Sequential Learning
* [ ] Optimal Errors and Phase Transitions in High-Dimensional Generalized Linear Models
* [ ] Exact and Robust Conformal Inference Methods for Predictive Machine Learning With Dependent Data
* [ ] Nonstochastic Bandits with Composite Anonymous Feedback
* [ ] Lower Bounds for Higher-Order Convex Optimization
* [ ] Log-concave sampling: Metropolis-Hastings algorithms are fast!
* [ ] Incentivizing Exploration by Heterogeneous Users
* [ ] Fast and Sample Near-Optimal Algorithms for Learning Multidimensional Histograms
* [ ] Time-Space Tradeoffs for Learning Finite Functions from Random Tests, with Applications to Polynomials
* [ ] Local Optimality and Generalization Guarantees for the Langevin Algorithm via Empirical Metastability
* [ ] Hardness of Learning Noisy Halfspaces using Polynomial Thresholds
* [ ] Best of Both Worlds: Stochastic & Adversarial Best-Arm Identification
* [ ] Learning Patterns for Detection with Multiscale Scan Statistics
* [ ] Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk
* [ ] Small-loss bounds for online learning with partial information
* [ ] Empirical bounds for functions with weak interactions
* [ ] Restricted Eigenvalue from Stable Rank with Applications to Sparse Linear Regression
* [ ] Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent
* [ ] Convex Optimization with Unbounded Nonconvex Oracles Using Simulated Annealing
* [ ] Learning Mixtures of Linear Regressions with Nearly Optimal Complexity
* [ ] Detecting Correlations with Little Memory and Communication
* [ ] Finite Sample Analysis of Two-Timescale Stochastic Approximation with Applications to Reinforcement Learning
* [ ] Near-Optimal Sample Complexity Bounds for Maximum Likelihood Estimation of Multivariate Log-concave Densities
* [ ] More Adaptive Algorithms for Adversarial Bandits
* [ ] Efficient Convex Optimization with Membership Oracles
* [ ] A General Approach to Multi-Armed Bandits Under Risk Criteria
* [ ] An Optimal Learning Algorithm for Online Unconstrained Submodular Maximization
* [ ] The Mean-Field Approximation: Information Inequalities, Algorithms, and Complexity
* [ ] Approximation beats concentration? An approximation view on inference with smooth radial kernels
* [ ] Non-Convex Matrix Completion Against a Semi-Random Adversary
* [ ] The Vertex Sample Complexity of Free Energy is Polynomial
* [ ] Efficient Algorithms for Outlier-Robust Regression
* [ ] Action-Constrained Markov Decision Processes With Kullback-Leibler Cost
* [ ] Fundamental Limits of Weak Recovery with Applications to Phase Retrieval
* [ ] Cutting plane methods can be extended into nonconvex optimization
* [ ] An Analysis of the t-SNE Algorithm for Data Visualization
* [ ] Adaptivity to Smoothness in X-armed bandits
* [ ] Black-Box Reductions for Parameter-free Online Learning in Banach Spaces
* [ ] A Data Prism: Semi-verified learning in the small-alpha regime
* [ ] Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations
* [ ] A Direct Sum for Information Learners
* [ ] Online learning over a finite action set with limited switching
* [ ] Smoothed Online Convex Optimization in High Dimensions via Online Balanced Descent
* [ ] Faster Rates for Convex-Concave Games
* [ ] Logistic Regression: The Importance of Being Improper
* [ ] L1 Regression using Lewis Weights Preconditioning and Stochastic Gradient Descent
* [ ] Optimal Single Sample Tests for Structured versus Unstructured Network Data
* [ ] A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation
* [ ] Privacy-preserving Prediction
* [ ] An Estimate Sequence for Geodesically Convex Optimization
* [ ] The Externalities of Exploration and How Data Diversity Helps Exploitation
* [ ] Efficient Contextual Bandits in Non-stationary Worlds
* [ ] Langevin Monte Carlo and JKO splitting
* [ ] Subpolynomial trace reconstruction for random strings and arbitrary deletion probability
* [ ] An explicit analysis of the entropic penalty in linear programming
* [ ] Efficient Active Learning of Sparse Halfspaces
* [ ] Marginal Singularity, and the Benefits of Labels in Covariate-Shift
* [ ] Learning Single Index Models in Gaussian Space
* [ ] Hidden Integrality and Exponential Rates of SDP Relaxations for Sub-Gaussian Mixture Models
* [ ] Counting Motifs with Graph Sampling
* [ ] Approximate Nearest Neighbors in Limited Space
* [ ] Breaking the $1/\sqrt{n}$ Barrier: Faster Rates for Permutation-based Models in Polynomial Time
* [ ] Reductions in Fair Learning and Optimization
* [ ] The Many Faces of Exponential Weights in Online Learning
* [ ] Reducibility and Computational Lower Bounds for Problems with Planted Sparse Structure
* [ ] Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem
* [ ] Online Learning: Sufficient Statistics and the Burkholder Method
* [ ] Minimax Bounds on Stochastic Batched Convex Optimization
* [ ] Geometric Lower Bounds for Distributed Parameter Estimation under Communication Constraints
* [ ] Local moment matching: A unified methodology for symmetric functional estimation and distribution estimation under Wasserstein distance
* [ ] Iterate averaging as regularization for stochastic gradient descent
* [ ] Smoothed Analysis for Efficient Semi-definite Programming
* [ ] Learning from Unreliable Datasets

## メンバー

現在1名
* 南 賢太郎 (@ktrmnm)
